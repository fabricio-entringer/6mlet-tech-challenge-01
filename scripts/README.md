# Books Scraper System

A comprehensive web scraping system to extract book data from [books.toscrape.com](https://books.toscrape.com/).

## Features

- âœ… **Complete Data Extraction**: Extracts all books from all categories with pagination support
- âœ… **Comprehensive Data Fields**: Captures title, price, rating, availability, category, and image URL
- âœ… **Robust Error Handling**: Implements retry mechanisms and graceful error recovery
- âœ… **Rate Limiting**: Respects website resources with configurable delays
- âœ… **CSV Export**: Stores data in well-structured CSV format
- âœ… **Detailed Logging**: Comprehensive logging for monitoring and debugging
- âœ… **Statistics**: Provides detailed statistics about scraped data
- âœ… **CLI Interface**: Easy-to-use command-line interface

## Installation

1. Install required dependencies:

```bash
pip install -r requirements.txt
```

1. The scraper is ready to use!

## Usage

### REST API Endpoints (NEW!)

The system now includes REST API endpoints for remote scraping operations:

#### Start the API Server
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

#### Available Endpoints

1. **Start Scraping**: `POST /scraping/start`
   ```bash
   curl -X POST "http://localhost:8000/scraping/start" \
     -H "Content-Type: application/json" \
     -d '{"delay": 1.0, "csv_filename": "api_books.csv"}'
   ```

2. **Get History**: `GET /scraping/history`
   ```bash
   curl "http://localhost:8000/scraping/history"
   ```

3. **Check Status**: `GET /scraping/status`
   ```bash
   curl "http://localhost:8000/scraping/status"
   ```

ðŸ“‹ **See [API_ENDPOINTS.md](../API_ENDPOINTS.md) for complete API documentation**

### Scraper API (Recommended)

The `scraper_api.py` provides both command line and programmatic interfaces:

#### Command Line Interface

```bash
# Navigate to the scripts directory
cd scripts

# Scrape all books (default mode)
python scraper_api.py

# Get available categories
python scraper_api.py --mode categories

# Scrape a specific category
python scraper_api.py --mode category --category "Fiction"

# Scrape a sample of books
python scraper_api.py --mode sample --max-books 100

# Customize scraping parameters
python scraper_api.py --delay 0.5 --max-retries 2 --timeout 15

# Skip saving to CSV
python scraper_api.py --no-save

# Show help
python scraper_api.py --help
```

#### Programmatic Usage

```python
from scraper_api import BooksScraperAPI

# Create API instance
api = BooksScraperAPI(delay=1.0, max_retries=3, timeout=10)

# Scrape all books
result = api.scrape_all_books()
print(f"Scraped {result['total_books']} books")
print(f"Categories: {result['total_categories']}")
print(f"Data saved to: {result['csv_file']}")

# Get available categories
categories = api.get_categories()
for cat in categories:
    print(f"- {cat['name']}: {cat['url']}")

# Scrape a specific category
fiction_result = api.scrape_category("Fiction")
print(f"Fiction books: {fiction_result['total_books']}")

# Scrape a sample
sample_result = api.scrape_sample(max_books=50)
print(f"Sample: {sample_result['total_books']} books")

# Get last scraping statistics
stats = api.get_scraper_stats()
print(f"Statistics: {stats}")
```

### Basic Usage

Run the scraper with default settings:

```bash
python scripts/run_scraper.py
```

### Advanced Usage

Customize scraper behavior:

```bash
# Custom delay and output settings
python scripts/run_scraper.py --delay 2.0 --output my_data --filename custom_books.csv

# Enable verbose logging
python scripts/run_scraper.py --verbose

# Custom retry and timeout settings
python scripts/run_scraper.py --max-retries 5 --timeout 15
```

### Direct Scraper Usage

```python
from scripts.books_scraper import BooksScraper

# Initialize scraper
scraper = BooksScraper(delay=1.0)

# Scrape all books
books = scraper.scrape_all_books()

# Save to CSV
output_file = scraper.save_to_csv("my_books.csv", "output")

# Get statistics
stats = scraper.get_statistics()
print(f"Total books: {stats['total_books']}")
```

## Configuration

The scraper can be configured through various parameters:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `delay` | 1.0 | Delay between requests (seconds) |
| `max_retries` | 3 | Maximum retry attempts for failed requests |
| `timeout` | 10 | Request timeout (seconds) |
| `output_dir` | "data" | Output directory for CSV files |

## Data Schema

The scraper extracts the following fields for each book:

| Field | Type | Description |
|-------|------|-------------|
| `title` | string | Book title |
| `price` | string | Book price (with currency symbol) |
| `rating_text` | string | Textual rating (One, Two, Three, Four, Five) |
| `rating_numeric` | integer | Numeric rating (1-5) |
| `availability` | string | Availability status |
| `category` | string | Book category |
| `image_url` | string | URL to book cover image |

## Output

The scraper generates:

1. **CSV File**: `data/books_data.csv` (default) containing all book data
2. **Log File**: `logs/scraper.log` with detailed operation logs
3. **Console Output**: Real-time progress and statistics

### Sample CSV Output

```csv
title,price,rating_text,rating_numeric,availability,category,image_url
"A Light in the Attic","Â£51.77","Three","3","In stock","Poetry","https://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg"
"Tipping the Velvet","Â£53.74","One","1","In stock","Historical Fiction","https://books.toscrape.com/media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg"
```

## Error Handling

The scraper implements robust error handling:

- **Network Errors**: Automatic retry with exponential backoff
- **Parsing Errors**: Graceful handling with error logging
- **Rate Limiting**: Configurable delays to respect server resources
- **Timeout Protection**: Prevents hanging requests

## Testing

Run the test suite:

```bash
# Run all tests
python -m pytest tests/test_books_scraper.py -v

# Run with coverage
python -m pytest tests/test_books_scraper.py --cov=scripts.books_scraper

# Run integration tests (requires network access)
python -m pytest tests/test_books_scraper.py -m integration
```

## Logging

The scraper provides detailed logging:

- **INFO**: Progress updates and statistics
- **WARNING**: Recoverable errors and retries
- **ERROR**: Critical errors and failures

Log files are saved as `logs/scraper.log` and also displayed in the console.

## Best Practices

The scraper follows web scraping best practices:

1. **Respectful Crawling**: Implements delays between requests
2. **Error Recovery**: Robust retry mechanisms
3. **Resource Efficiency**: Reuses HTTP sessions
4. **User Agent**: Identifies itself appropriately
5. **Graceful Degradation**: Continues operation despite individual failures

## Performance

Typical performance metrics:

- **Speed**: ~1-2 seconds per page (with 1s delay)
- **Coverage**: 100% of available books
- **Reliability**: >99% success rate with retry logic
- **Memory**: Efficient streaming processing

## Limitations

- **Rate Limiting**: Respects 1-second delay by default
- **Network Dependent**: Requires stable internet connection
- **Site Structure**: Depends on current HTML structure of books.toscrape.com

## Troubleshooting

### Common Issues

1. **Connection Errors**:
   - Check internet connection
   - Increase timeout settings
   - Verify website accessibility

2. **Incomplete Data**:
   - Check log files for specific errors
   - Verify HTML structure hasn't changed
   - Increase retry attempts

3. **Performance Issues**:
   - Reduce delay for faster scraping (if appropriate)
   - Monitor system resources
   - Check network latency

### Debug Mode

Enable verbose logging for debugging:

```bash
python scripts/run_scraper.py --verbose
```

## Contributing

When contributing to the scraper:

1. Follow PEP 8 style guidelines
2. Add comprehensive docstrings
3. Include unit tests for new features
4. Update this README for new functionality
5. Test with various network conditions

## License

This scraper is part of the 6mlet-tech-challenge-01 project and follows the project's license terms.
